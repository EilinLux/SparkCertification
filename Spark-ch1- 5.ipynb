{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c820b60e",
   "metadata": {},
   "source": [
    "###  DOCUMENTATION:  \n",
    "    https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c7c9b",
   "metadata": {},
   "source": [
    "### Initiate Spark \n",
    "\n",
    "control a Spark Application through a driver process called SparkSession\n",
    "\n",
    "1) on console: ```spark```\n",
    "\n",
    "2) on jupyternotebook: ```jupyter notebook``` then in a cell run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b82b40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58abcc2",
   "metadata": {},
   "source": [
    "### Spark UI \n",
    "\n",
    "```http://localhost:4040/jobs/```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f546b4d0",
   "metadata": {},
   "source": [
    "#todo to 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8eba3",
   "metadata": {},
   "source": [
    "## Ch5: Basic Structured Operations\n",
    "\n",
    "### Dataframes Schemas\n",
    "\n",
    "1) schema-on-read (autodetect)\n",
    "\n",
    "2) defined explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56d5e2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to load and check the schema  without schema(myManulaSchema)\n",
    "spark.read.format('json').load('./data/flight-data/json/2015-summary.json').schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e887f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "flights_df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
    "  .load(\"./data/flight-data/json/2015-summary.json\")\n",
    "\n",
    "flights_df .schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925c991f",
   "metadata": {},
   "source": [
    "A schema is a ```StructType``` build by ```StructField``` made of:\n",
    "\n",
    "    1) nameColumn\n",
    "    \n",
    "    2) typeColumn\n",
    "    \n",
    "    3) Nullable\n",
    "    \n",
    "    4) metadata (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61162e",
   "metadata": {},
   "source": [
    "To check the schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a3c2eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912bbec0",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797fcab",
   "metadata": {},
   "source": [
    "The book combines the Scala and PySpark API's.\n",
    "\n",
    "In Scala / Java API, ```df.col(\"column_name\")```,```df.col('column_name)```,```df(\"column_name\")``` or  ```df.apply(\"column_name\")``` return the Column.\n",
    "\n",
    "Whereas in pyspark use the below to get the column from DF.\n",
    "\n",
    "```df.colName```\n",
    "```df[\"colName\"]```\n",
    "\n",
    "<b>HOWEVER </b>, if using  ```select``` it is also possible to use ```col(\"column_name\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a75543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'DEST_COUNTRY_NAME'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "\n",
    "# df(\"someColumnName\")\n",
    "flights_df[\"DEST_COUNTRY_NAME\"]\n",
    "flights_df.DEST_COUNTRY_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "939b2ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.columns #column property to access columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e9c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "697042f5",
   "metadata": {},
   "source": [
    "## Expressions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed6afe",
   "metadata": {},
   "source": [
    "an expression parses transformations and column references from a string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a57fb6db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(((((number + 5) * 200) - 6) < 5)=False),\n",
       " Row(((((number + 5) * 200) - 6) < 5)=False),\n",
       " Row(((((number + 5) * 200) - 6) < 5)=False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df = spark.range(500).toDF(\"number\")\n",
    "df.select(df[\"number\"] + 10).take(3)\n",
    " \n",
    "df.select(expr(\"(((number + 5) * 200) - 6) < 5\")).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b4794",
   "metadata": {},
   "source": [
    "### Rows\n",
    "Each row is a single record, represented as an object of type ```Row```. To manipulate an object of type ```Row``` use a column expression (previous paragraph). Internally represent arrays of bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b8d446b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(number=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first() # an example to check a type Row is printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc9043",
   "metadata": {},
   "source": [
    "#### Create Rows\n",
    "1) manually instanciatin an object ```Row``` (values in the same order and type as the schema of the df to which you have to append them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f98f8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\", None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8211a591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow[0] # to access the value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49606a8c",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9c090",
   "metadata": {},
   "source": [
    "#### Creating df\n",
    "1) from a file / raw data sources ```spark.read.format('format').source('path/to/data')```\n",
    "2) from a set of rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e00ffc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|Welcome|None|number|\n",
      "+-------+----+------+\n",
      "|  Hello|null|     1|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"Welcome\", StringType(), True),\n",
    "  StructField(\"None\", StringType(), True),\n",
    "  StructField(\"number\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcbc7c4",
   "metadata": {},
   "source": [
    "#### Transforming a Df\n",
    "To transform a Df we can only manipulate columns (rows singularly are not accessible) and we can use \n",
    "1) ```select``` method\n",
    "\n",
    "2) ```selectExpr``` method\n",
    "\n",
    "3) ```import pyspark.sql.functions``` package\n",
    "\n",
    "#### Transforming using SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aadd23de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.select(\"DEST_COUNTRY_NAME\").show(2) # singular selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79108fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2) #multiple selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04b961b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "|    United States|            Ireland|\n",
      "|            Egypt|      United States|\n",
      "|    United States|              India|\n",
      "+-----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, column, col \n",
    "flights_df.select(expr(\"DEST_COUNTRY_NAME\"),\n",
    "                 col(\"ORIGIN_COUNTRY_NAME\"),\n",
    "#                  column(\"count\")               # column is not working \n",
    "                 ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c38d4",
   "metadata": {},
   "source": [
    "<b> NOT TRUE common mistake </b>: use a mix of column Objects and strings, does not give an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8612937c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|ORIGIN_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-------------------+-----------------+\n",
      "|            Romania|    United States|\n",
      "|            Croatia|    United States|\n",
      "|            Ireland|    United States|\n",
      "|      United States|            Egypt|\n",
      "|              India|    United States|\n",
      "+-------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, column, col \n",
    "flights_df.select(\n",
    "                 col(\"ORIGIN_COUNTRY_NAME\"),\"DEST_COUNTRY_NAME\"\n",
    "                 ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c32b7f",
   "metadata": {},
   "source": [
    "#### Rename columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57d41862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| origin|\n",
      "+-------+\n",
      "|Romania|\n",
      "|Croatia|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+\n",
      "|origin2|\n",
      "+-------+\n",
      "|Romania|\n",
      "|Croatia|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.select(expr(\"ORIGIN_COUNTRY_NAME AS origin\")).show(2)\n",
    "flights_df.select(expr(\"ORIGIN_COUNTRY_NAME\").alias(\"origin2\")).show(2) #### NB the alias is INSIDE select"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087fad6",
   "metadata": {},
   "source": [
    "#### Transforming using .selectExpr()\n",
    "Because ```select``` and ```expr``` is a commone pattern --> short hand ```selectExpr```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37793bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| origin|\n",
      "+-------+\n",
      "|Romania|\n",
      "|Croatia|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.selectExpr(\"ORIGIN_COUNTRY_NAME AS origin\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6460724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| origin|withInCountry|\n",
      "+-----------------+-------------------+-----+-------+-------------+\n",
      "|    United States|            Romania|   15|Romania|        false|\n",
      "|    United States|            Croatia|    1|Croatia|        false|\n",
      "+-----------------+-------------------+-----+-------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.selectExpr(\"*\",\"ORIGIN_COUNTRY_NAME AS origin\", \"ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as withInCountry\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d0f4f",
   "metadata": {},
   "source": [
    "###  Literals\n",
    "```lit``` is used to pass explicit values into Spark that are just a value. Need to be imported "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f86f3c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|true|\n",
      "+-----------------+-------------------+-----+----+\n",
      "|    United States|            Romania|   15|true|\n",
      "|    United States|            Croatia|    1|true|\n",
      "+-----------------+-------------------+-----+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+-----------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|True value?|\n",
      "+-----------------+-------------------+-----+-----------+\n",
      "|    United States|            Romania|   15|       true|\n",
      "|    United States|            Croatia|    1|       true|\n",
      "+-----------------+-------------------+-----+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit \n",
    "flights_df.select(expr(\"*\"), lit(True)).show(2) # lit is OUTSIDE expr()\n",
    "flights_df.select(expr(\"*\"), lit(True).alias(\"True value?\")).show(2) # lit is OUTSIDE expr()\n",
    "\n",
    "#NB a difference with SCALA is that .alias() in Python is like .as() in Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c36b8f9",
   "metadata": {},
   "source": [
    "### Adding Columns with ```withColumns('column_name', value)``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3052d14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.withColumn('numberOne', lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b39a8343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withInCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumn(column_name, expression)\n",
    "flights_df.withColumn('withInCountry', expr(\"DEST_COUNTRY_NAME=ORIGIN_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dba717",
   "metadata": {},
   "source": [
    "### Rename a column ```withColumnRenamed('old_name', 'new_name')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7601a0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----+\n",
      "|  destination|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------+-------------------+-----+\n",
      "|United States|            Romania|   15|\n",
      "|United States|            Croatia|    1|\n",
      "+-------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_flights_df= flights_df.withColumnRenamed(\"DEST_COUNTRY_NAME\", 'destination')\n",
    "new_flights_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee66eb4",
   "metadata": {},
   "source": [
    "### Remove columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b57ba88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Romania|   15|\n",
      "|            Croatia|    1|\n",
      "+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_flights_df=new_flights_df.drop('destination')\n",
    "new_flights_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cce892",
   "metadata": {},
   "source": [
    "### Changing Column Type ```cast('type')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af6e6ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------------+\n",
      "|ORIGIN_COUNTRY_NAME|count|StringNumber|\n",
      "+-------------------+-----+------------+\n",
      "|            Romania|   15|          15|\n",
      "|            Croatia|    1|           1|\n",
      "+-------------------+-----+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- StringNumber: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_flights_df.withColumn(\"StringNumber\", col('count').cast('string')).show(2)\n",
    "new_flights_df.withColumn(\"StringNumber\", col('count').cast('string')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d160e9",
   "metadata": {},
   "source": [
    "### Filtering ```where()``` or ```filter()```\n",
    "\n",
    "There are two methods to perform this operation: you can use where or filter\n",
    "and they both will perform the same operation and accept the same argument types when used\n",
    "with DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f18e74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Croatia|    1|\n",
      "|          Singapore|    1|\n",
      "+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_flights_df.filter(col(\"count\") < 2 ).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cab01ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Croatia|    1|\n",
      "|          Singapore|    1|\n",
      "+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Croatia|    1|\n",
      "|          Singapore|    1|\n",
      "+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_flights_df.where(col(\"count\") < 2 ).show(2)\n",
    "new_flights_df.where(expr(\"count\")< 2 ).show(2)\n",
    "# new_flights_df.where(\"count\"< 2 ).show(2) # NOT working is comapring strign and number "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7447203a",
   "metadata": {},
   "source": [
    "<b> NOT use multiple filters into the same expression.</b> Although this is\n",
    "possible, it is not always useful, because Spark automatically performs all filtering operations at\n",
    "the same time regardless of the filter ordering. This means that if you want to specify multiple\n",
    "AND filters, <b> just chain them sequentially "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1182d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Romania|   15|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_flights_df.where(col(\"count\") >2).where(col(\"ORIGIN_COUNTRY_NAME\") == \"Romania\" ).show(7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5462ad",
   "metadata": {},
   "source": [
    "### Getting Unique Rows: df.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bd31aef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_flights_df.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "755ead4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|          Lithuania|    1|\n",
      "|          Singapore|    1|\n",
      "|          Gibraltar|    1|\n",
      "|           Bulgaria|    1|\n",
      "|            Namibia|    1|\n",
      "+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_flights_df.distinct().orderBy(\"count\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a772738f",
   "metadata": {},
   "source": [
    "### Filtering by Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a208fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(ORIGIN_COUNTRY_NAME='India', count=62),\n",
       " Row(ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
       " Row(ORIGIN_COUNTRY_NAME='United States', count=588),\n",
       " Row(ORIGIN_COUNTRY_NAME='United States', count=40),\n",
       " Row(ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_flights_df.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41092861",
   "metadata": {},
   "source": [
    ".take() results in an Array of Rows. This is an action and performs collecting the data (like collect does).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6bb38709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.limit(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bde17c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.limit(10).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d95ff78",
   "metadata": {},
   "source": [
    "limit() results in a new Dataframe. This is a transformation and does not perform collecting the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f14431",
   "metadata": {},
   "source": [
    "### Random Sample: df.sample()\n",
    "sample some random records from your DataFrame. sample(withReplacement=None, fraction=None, seed=None)\n",
    "This is not guaranteed to provide exactly the fraction specified of the total count of the given DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d7ab010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in Python\n",
    "seed = 5   \n",
    "withReplacement = False #Sample with replacement or not (default False).\n",
    "fraction = 0.5  # hFraction of rows to generate, range [0.0, 1.0].\n",
    "new_flights_df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79deb83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_flights_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a2c0617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_flights_df.count()*fraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eec2d13",
   "metadata": {},
   "source": [
    "### Random Splits .randomSplit([0.25, 0.75], seed)\n",
    "Random splits can be helpful when you need to break up your DataFrame into a random “splits” of the original DataFrame. \n",
    "\n",
    "Parameters:\n",
    "\n",
    "weights: list --> list of doubles as weights with which to split the DataFrame. Weights will be normalized if they don’t sum up to 1.0.\n",
    "\n",
    "seed: int, optional  --> \n",
    "The seed for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "db0a8f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|             Angola|   13|\n",
      "|           Anguilla|   38|\n",
      "|          Australia|  258|\n",
      "+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrames = new_flights_df.randomSplit([0.25, 0.75], seed)\n",
    "dataFrames[0].show(3)\n",
    "dataFrames[0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aaaeeb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|Antigua and Barbuda|  117|\n",
      "|          Argentina|  141|\n",
      "|              Aruba|  342|\n",
      "+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "185"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrames[1].show(3)\n",
    "dataFrames[1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce727416",
   "metadata": {},
   "source": [
    "### Concatenating and Appending Rows (Union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cad4140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|      New Country|      Other Country|    5|\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# in Python\n",
    "from pyspark.sql import Row\n",
    "schema = flights_df.schema\n",
    "newRows = [\n",
    "Row(\"New Country\", \"Other Country\", 5), Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "#Just drop the L; all integers in Python 3 are long. What was long in Python 2 is now the standard int type in Python 3.\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "82b1b9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|      New Country|      Other Country|    5|\n",
      "|    New Country 2|    Other Country 3|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reduced = flights_df.limit(2)\n",
    "reduced.union(newDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cec0147",
   "metadata": {},
   "source": [
    "### Sorting Rows\n",
    "\n",
    "sort\n",
    "and orderBy that work the exact same way. They accept both column expressions and strings as\n",
    "well as multiple columns. The default is to sort in ascending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d51c984b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+--------------------+-------------------+------+\n",
      "|       United States|            Vietnam|     2|\n",
      "|       United States|          Venezuela|   246|\n",
      "|       United States|            Uruguay|    13|\n",
      "|              Zambia|      United States|     1|\n",
      "|           Venezuela|      United States|   290|\n",
      "|             Uruguay|      United States|    43|\n",
      "|       United States|      United States|370002|\n",
      "|      United Kingdom|      United States|  2025|\n",
      "|United Arab Emirates|      United States|   320|\n",
      "|             Ukraine|      United States|    14|\n",
      "|Turks and Caicos ...|      United States|   230|\n",
      "|              Turkey|      United States|   138|\n",
      "|             Tunisia|      United States|     3|\n",
      "| Trinidad and Tobago|      United States|   211|\n",
      "|         The Bahamas|      United States|   955|\n",
      "|            Thailand|      United States|     3|\n",
      "|              Taiwan|      United States|   266|\n",
      "|         Switzerland|      United States|   294|\n",
      "|              Sweden|      United States|   118|\n",
      "|            Suriname|      United States|     1|\n",
      "+--------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.orderBy([\"ORIGIN_COUNTRY_NAME\",\"DEST_COUNTRY_NAME\"], ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b17e5258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Vietnam|    2|\n",
      "|    United States|          Venezuela|  246|\n",
      "|    United States|            Uruguay|   13|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.sort([\"ORIGIN_COUNTRY_NAME\",\"DEST_COUNTRY_NAME\"], ascending=False).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5333f1c",
   "metadata": {},
   "source": [
    "### asc_nulls_first, desc_nulls_first, asc_nulls_last, or desc_nulls_last \n",
    "\n",
    "to specify where you would like your null values to appear in an ordered\n",
    "DataFrame. Returns a sort expression based on ascending order of the column, and null values return before non-null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b33a09af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|          Algeria|      United States|    4|\n",
      "|           Angola|      United States|   15|\n",
      "|         Anguilla|      United States|   41|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the syntax df.orderBy(df.column.asc_nulls_last())\n",
    "\n",
    "flights_df.orderBy(flights_df.DEST_COUNTRY_NAME.asc_nulls_last()).show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa58f37",
   "metadata": {},
   "source": [
    "### sortWithinPartition vs orderBy vs sort \n",
    "\n",
    "The documentation of sortWithinPartition states it returns a new Dataset with each partition sorted by the given expressions\n",
    "\n",
    "The easiest way to think of this function is to imagine a fourth column (the partition id) that is used as primary sorting criterion. The function spark_partition_id() prints the partition.\n",
    "\n",
    "For example if you have just one large partition (something that you as a Spark user would never do!), sortWithinPartition works as a normal sort:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8bcf138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+---------+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|partition|\n",
      "+--------------------+-------------------+------+---------+\n",
      "|       United States|            Vietnam|     2|        0|\n",
      "|       United States|          Venezuela|   246|        0|\n",
      "|       United States|            Uruguay|    13|        0|\n",
      "|              Zambia|      United States|     1|        0|\n",
      "|           Venezuela|      United States|   290|        0|\n",
      "|             Uruguay|      United States|    43|        0|\n",
      "|       United States|      United States|370002|        0|\n",
      "|      United Kingdom|      United States|  2025|        0|\n",
      "|United Arab Emirates|      United States|   320|        0|\n",
      "|             Ukraine|      United States|    14|        0|\n",
      "|Turks and Caicos ...|      United States|   230|        0|\n",
      "|              Turkey|      United States|   138|        0|\n",
      "|             Tunisia|      United States|     3|        0|\n",
      "| Trinidad and Tobago|      United States|   211|        0|\n",
      "|         The Bahamas|      United States|   955|        0|\n",
      "|            Thailand|      United States|     3|        0|\n",
      "|              Taiwan|      United States|   266|        0|\n",
      "|         Switzerland|      United States|   294|        0|\n",
      "|              Sweden|      United States|   118|        0|\n",
      "|            Suriname|      United States|     1|        0|\n",
      "+--------------------+-------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import spark_partition_id\n",
    "# This is non deterministic because it depends on data partitioning and task scheduling.\n",
    "#https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.spark_partition_id.html\n",
    "\n",
    "\n",
    "flights_df.repartition(1)\\\n",
    "        .sortWithinPartitions([\"ORIGIN_COUNTRY_NAME\",\"DEST_COUNTRY_NAME\"],\\\n",
    "                                               ascending=False)\\\n",
    "        .withColumn(\"partition\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d96609",
   "metadata": {},
   "source": [
    "\n",
    "If there are more partitions, the results are only sorted within each partition:\n",
    "\n",
    "\n",
    "Why would one use sortWithPartition instead of sort? \n",
    "\n",
    "<b> sortWithPartition does not trigger a shuffle </b>, as the data is only moved within the executors. sort however will trigger a shuffle. Therefore sortWithPartition executes faster. If the data is partitioned by a meaningful column, sorting within each partition might be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8a1c167a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----+---------+\n",
      "|  DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|partition|\n",
      "+-------------------+-------------------+-----+---------+\n",
      "|   Marshall Islands|      United States|   42|        0|\n",
      "|      United States|              Italy|  438|        0|\n",
      "|      United States|           Anguilla|   38|        0|\n",
      "|            Jamaica|      United States|  666|        1|\n",
      "|            Hungary|      United States|    2|        1|\n",
      "|      United States|              Qatar|  109|        1|\n",
      "|         Luxembourg|      United States|  155|        2|\n",
      "|              India|      United States|   61|        2|\n",
      "|      United States|       Cook Islands|   13|        2|\n",
      "|     United Kingdom|      United States| 2025|        3|\n",
      "|           Kiribati|      United States|   26|        3|\n",
      "|      United States|          Argentina|  141|        3|\n",
      "|            Uruguay|      United States|   43|        4|\n",
      "|         Guadeloupe|      United States|   56|        4|\n",
      "|      French Guiana|      United States|    5|        4|\n",
      "|Antigua and Barbuda|      United States|  126|        5|\n",
      "|      United States|              Malta|    2|        5|\n",
      "|      United States|             Greece|   23|        5|\n",
      "|           Bulgaria|      United States|    3|        6|\n",
      "|      United States|           Kiribati|   35|        6|\n",
      "+-------------------+-------------------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.repartition(90)\\\n",
    "        .sortWithinPartitions([\"ORIGIN_COUNTRY_NAME\",\"DEST_COUNTRY_NAME\"],\\\n",
    "                                               ascending=False)\\\n",
    "        .withColumn(\"partition\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc21ca",
   "metadata": {},
   "source": [
    "### Repartition\n",
    "\n",
    "Repartition will incur a full shuffle of the data, regardless of whether one is necessary. This means that you should typically only repartition when the future number of partitions is greater than your current number of partitions.\n",
    "\n",
    "\n",
    "If you know that you’re going to be filtering by a certain column often, it can be worth\n",
    "repartitioning based on that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b5a29ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in Python\n",
    "flights_df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed020dc",
   "metadata": {},
   "source": [
    "### Coalesce\n",
    "will not incur a full shuffle and will try to combine partitions. This operation will shuffle your data into five partitions based on the destination country name, and then coalesce them (without a full shuffle):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python\n",
    "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87c10b5",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7aa43913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object _local_iterator_from_socket.<locals>.PyLocalIterable.__iter__ at 0x7f21ff565510>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in Python\n",
    "collectDF = flights_df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count\n",
    "collectDF.show() # this prints it out nicely\n",
    "collectDF.show(5, False)\n",
    "collectDF.collect()\n",
    "collectDF.toLocalIterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b9716996",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'StorageLevel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-71aac2126eef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollectDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStorageLevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \"\"\"\n\u001b[1;32m   1642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1643\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1644\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'StorageLevel'"
     ]
    }
   ],
   "source": [
    "collectDF.StorageLevel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3bb1ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Romania|   15|\n",
      "|            Croatia|    1|\n",
      "|            Ireland|  344|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_flights_df.limit(3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b1cbca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_flights_df.where(col(\"count\") >2 | col(\"ORIGIN_COUNTRY_NAME\") == \"Romania\" ).show(7)\n",
    "#use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc869209",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967820c",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a45f9",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d7d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c0aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d51f8884",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Undefined function: 'column'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-4d2062ca1afb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"(((column('number') + 5) * 200) - 6) < 5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \"\"\"\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Undefined function: 'column'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 3"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"(((column('number') + 5) * 200) - 6) < 5\")).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1480f449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
