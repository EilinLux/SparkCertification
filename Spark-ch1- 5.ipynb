{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1c7c9b",
   "metadata": {},
   "source": [
    "### Initiate Spark \n",
    "\n",
    "control a Spark Application through a driver process called SparkSession\n",
    "\n",
    "1) on console: ```spark```\n",
    "\n",
    "2) on jupyternotebook: ```jupyter notebook``` then in a cell run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b82b40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58abcc2",
   "metadata": {},
   "source": [
    "### Spark UI \n",
    "\n",
    "```http://localhost:4040/jobs/```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f546b4d0",
   "metadata": {},
   "source": [
    "#todo to 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8eba3",
   "metadata": {},
   "source": [
    "## Ch5: Basic Structured Operations\n",
    "\n",
    "### Dataframes Schemas\n",
    "\n",
    "1) schema-on-read (autodetect)\n",
    "\n",
    "2) defined explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "56d5e2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to load and check the schema  without schema(myManulaSchema)\n",
    "spark.read.format('json').load('./data/flight-data/json/2015-summary.json').schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9e887f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "flights_df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
    "  .load(\"./data/flight-data/json/2015-summary.json\")\n",
    "\n",
    "flights_df .schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925c991f",
   "metadata": {},
   "source": [
    "A schema is a ```StructType``` build by ```StructField``` made of:\n",
    "\n",
    "    1) nameColumn\n",
    "    \n",
    "    2) typeColumn\n",
    "    \n",
    "    3) Nullable\n",
    "    \n",
    "    4) metadata (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61162e",
   "metadata": {},
   "source": [
    "To check the schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a3c2eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912bbec0",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3797fcab",
   "metadata": {},
   "source": [
    "The book combines the Scala and PySpark API's.\n",
    "\n",
    "In Scala / Java API, ```df.col(\"column_name\")```,```df.col('column_name)```,```df(\"column_name\")``` or  ```df.apply(\"column_name\")``` return the Column.\n",
    "\n",
    "Whereas in pyspark use the below to get the column from DF.\n",
    "\n",
    "```df.colName```\n",
    "```df[\"colName\"]```\n",
    "\n",
    "<b>HOWEVER </b>, if using  ```select``` it is also possible to use ```col(\"column_name\")```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "64a75543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'DEST_COUNTRY_NAME'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, column\n",
    "\n",
    "# df(\"someColumnName\")\n",
    "flights_df[\"DEST_COUNTRY_NAME\"]\n",
    "flights_df.DEST_COUNTRY_NAME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "939b2ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.columns #column property to access columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145e9c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "697042f5",
   "metadata": {},
   "source": [
    "## Expressions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed6afe",
   "metadata": {},
   "source": [
    "an expression parses transformations and column references from a string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a57fb6db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(((((number + 5) * 200) - 6) < 5)=False),\n",
       " Row(((((number + 5) * 200) - 6) < 5)=False),\n",
       " Row(((((number + 5) * 200) - 6) < 5)=False)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df = spark.range(500).toDF(\"number\")\n",
    "df.select(df[\"number\"] + 10).take(3)\n",
    " \n",
    "df.select(expr(\"(((number + 5) * 200) - 6) < 5\")).take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b4794",
   "metadata": {},
   "source": [
    "### Rows\n",
    "Each row is a single record, represented as an object of type ```Row```. To manipulate an object of type ```Row``` use a column expression (previous paragraph). Internally represent arrays of bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b8d446b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(number=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first() # an example to check a type Row is printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dc9043",
   "metadata": {},
   "source": [
    "#### Create Rows\n",
    "1) manually instanciatin an object ```Row``` (values in the same order and type as the schema of the df to which you have to append them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f98f8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\", None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8211a591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow[0] # to access the value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49606a8c",
   "metadata": {},
   "source": [
    "### DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9c090",
   "metadata": {},
   "source": [
    "#### Creating df\n",
    "1) from a file / raw data sources ```spark.read.format('format').source('path/to/data')```\n",
    "2) from a set of rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e00ffc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------+\n",
      "|Welcome|None|number|\n",
      "+-------+----+------+\n",
      "|  Hello|null|     1|\n",
      "+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"Welcome\", StringType(), True),\n",
    "  StructField(\"None\", StringType(), True),\n",
    "  StructField(\"number\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])\n",
    "\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcbc7c4",
   "metadata": {},
   "source": [
    "#### Transforming a Df\n",
    "To transform a Df we can only manipulate columns (rows singularly are not accessible) and we can use \n",
    "1) ```select``` method\n",
    "\n",
    "2) ```selectExpr``` method\n",
    "\n",
    "3) ```import pyspark.sql.functions``` package\n",
    "\n",
    "#### Transforming using SELECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aadd23de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.select(\"DEST_COUNTRY_NAME\").show(2) # singular selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "79108fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2) #multiple selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "04b961b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "|    United States|            Ireland|\n",
      "|            Egypt|      United States|\n",
      "|    United States|              India|\n",
      "+-----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, column, col \n",
    "flights_df.select(expr(\"DEST_COUNTRY_NAME\"),\n",
    "                 col(\"ORIGIN_COUNTRY_NAME\"),\n",
    "#                  column(\"count\")               # column is not working \n",
    "                 ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c38d4",
   "metadata": {},
   "source": [
    "<b> NOT TRUE common mistake </b>: use a mix of column Objects and strings, does not give an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8612937c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|ORIGIN_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-------------------+-----------------+\n",
      "|            Romania|    United States|\n",
      "|            Croatia|    United States|\n",
      "|            Ireland|    United States|\n",
      "|      United States|            Egypt|\n",
      "|              India|    United States|\n",
      "+-------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, column, col \n",
    "flights_df.select(\n",
    "                 col(\"ORIGIN_COUNTRY_NAME\"),\"DEST_COUNTRY_NAME\"\n",
    "                 ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c32b7f",
   "metadata": {},
   "source": [
    "#### Rename columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "57d41862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| origin|\n",
      "+-------+\n",
      "|Romania|\n",
      "|Croatia|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------+\n",
      "|origin2|\n",
      "+-------+\n",
      "|Romania|\n",
      "|Croatia|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.select(expr(\"ORIGIN_COUNTRY_NAME AS origin\")).show(2)\n",
    "flights_df.select(expr(\"ORIGIN_COUNTRY_NAME\").alias(\"origin2\")).show(2) #### NB the alias is INSIDE select"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087fad6",
   "metadata": {},
   "source": [
    "#### Transforming using .selectExpr()\n",
    "Because ```select``` and ```expr``` is a commone pattern --> short hand ```selectExpr```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "37793bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| origin|\n",
      "+-------+\n",
      "|Romania|\n",
      "|Croatia|\n",
      "+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.selectExpr(\"ORIGIN_COUNTRY_NAME AS origin\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6460724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| origin|withInCountry|\n",
      "+-----------------+-------------------+-----+-------+-------------+\n",
      "|    United States|            Romania|   15|Romania|        false|\n",
      "|    United States|            Croatia|    1|Croatia|        false|\n",
      "+-----------------+-------------------+-----+-------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.selectExpr(\"*\",\"ORIGIN_COUNTRY_NAME AS origin\", \"ORIGIN_COUNTRY_NAME = DEST_COUNTRY_NAME as withInCountry\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d0f4f",
   "metadata": {},
   "source": [
    "###  Literals\n",
    "```lit``` is used to pass explicit values into Spark that are just a value. Need to be imported "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f86f3c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|true|\n",
      "+-----------------+-------------------+-----+----+\n",
      "|    United States|            Romania|   15|true|\n",
      "|    United States|            Croatia|    1|true|\n",
      "+-----------------+-------------------+-----+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-----------------+-------------------+-----+-----------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|True value?|\n",
      "+-----------------+-------------------+-----+-----------+\n",
      "|    United States|            Romania|   15|       true|\n",
      "|    United States|            Croatia|    1|       true|\n",
      "+-----------------+-------------------+-----+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit \n",
    "flights_df.select(expr(\"*\"), lit(True)).show(2) # lit is OUTSIDE expr()\n",
    "flights_df.select(expr(\"*\"), lit(True).alias(\"True value?\")).show(2) # lit is OUTSIDE expr()\n",
    "\n",
    "#NB a difference with SCALA is that .alias() in Python is like .as() in Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c36b8f9",
   "metadata": {},
   "source": [
    "### Adding Columns with ```withColumns('column_name', value)``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3052d14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights_df.withColumn('numberOne', lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b39a8343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withInCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumn(column_name, expression)\n",
    "flights_df.withColumn('withInCountry', expr(\"DEST_COUNTRY_NAME=ORIGIN_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dba717",
   "metadata": {},
   "source": [
    "### Rename a column ```withColumnRenamed('old_name', 'new_name')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7601a0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-----+\n",
      "|  destination|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------+-------------------+-----+\n",
      "|United States|            Romania|   15|\n",
      "|United States|            Croatia|    1|\n",
      "+-------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_flights_df= flights_df.withColumnRenamed(\"DEST_COUNTRY_NAME\", 'destination')\n",
    "new_flights_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee66eb4",
   "metadata": {},
   "source": [
    "### Remove columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b57ba88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Romania|   15|\n",
      "|            Croatia|    1|\n",
      "+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_flights_df=new_flights_df.drop('destination')\n",
    "new_flights_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cce892",
   "metadata": {},
   "source": [
    "### Changing Column Type ```cast('type')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "af6e6ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------------+\n",
      "|ORIGIN_COUNTRY_NAME|count|StringNumber|\n",
      "+-------------------+-----+------------+\n",
      "|            Romania|   15|          15|\n",
      "|            Croatia|    1|           1|\n",
      "+-------------------+-----+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      " |-- StringNumber: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_flights_df.withColumn(\"StringNumber\", col('count').cast('string')).show(2)\n",
    "new_flights_df.withColumn(\"StringNumber\", col('count').cast('string')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d160e9",
   "metadata": {},
   "source": [
    "### Filtering ```where()``` or ```filter()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2f18e74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Croatia|    1|\n",
      "|          Singapore|    1|\n",
      "+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_flights_df.filter(col(\"count\") < 2 ).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3cab01ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Croatia|    1|\n",
      "|          Singapore|    1|\n",
      "+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Croatia|    1|\n",
      "|          Singapore|    1|\n",
      "+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_flights_df.where(col(\"count\") < 2 ).show(2)\n",
    "new_flights_df.where(expr(\"count\")< 2 ).show(2)\n",
    "# new_flights_df.where(\"count\"< 2 ).show(2) # NOT working is comapring strign and number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b3bb1ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Romania|   15|\n",
      "|            Croatia|    1|\n",
      "|            Ireland|  344|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_flights_df.limit(3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce727416",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc869209",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967820c",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a45f9",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d7d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c0aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d51f8884",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Undefined function: 'column'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-4d2062ca1afb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"(((column('number') + 5) * 200) - 6) < 5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1667\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m         \"\"\"\n\u001b[0;32m-> 1669\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Undefined function: 'column'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 1 pos 3"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"(((column('number') + 5) * 200) - 6) < 5\")).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1480f449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
